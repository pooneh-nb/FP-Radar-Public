{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "linear-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install StellarGraph if running on Google Colab\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worth-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "\n",
    "try:\n",
    "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
    "except AttributeError:\n",
    "    raise ValueError(\n",
    "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
    "    ) from None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hourly-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.datasets import IAEnronEmployees\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-jaguar",
   "metadata": {},
   "source": [
    "# Load DataSet 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_train = pd.read_csv(\"/home/c6/Desktop/OpenWPM/jsons/CTDNE/2019/train/edges.csv\")\n",
    "with open(\"/home/c6/Desktop/OpenWPM/jsons/CTDNE/2019/train/Graph.json\") as json_data:\n",
    "  graph_train_nx = json_graph.node_link_graph(json.load(json_data))\n",
    "\n",
    "edges_test = pd.read_csv(\"/home/c6/Desktop/OpenWPM/jsons/CTDNE/2019/test/edges.csv\")\n",
    "with open(\"/home/c6/Desktop/OpenWPM/jsons/CTDNE/2019/test/Graph.json\") as json_data:\n",
    "  graph_test_nx = json_graph.node_link_graph(json.load(json_data))\n",
    "\n",
    "print(\n",
    "    f\"Number of edges in training graph: {len(graph_train_nx.edges())}\\n\"\n",
    "    f\"Number of edges in training set: {len(edges_train)}\\n\"\n",
    "    f\"Number of edges in test graph: {len(graph_test_nx.edges())}\\n\"\n",
    "    f\"Number of edges in test set: {len(edges_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-conditioning",
   "metadata": {},
   "source": [
    "## Create a StellarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_Stella = StellarGraph.from_networkx(graph_train_nx)\n",
    "graph_test_Stella = StellarGraph.from_networkx(graph_test_nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train = StellarGraph(\n",
    "    nodes=pd.DataFrame(index=graph_train_Stella.nodes()),\n",
    "    edges=edges_train,\n",
    "    edge_weight_column=\"time\",\n",
    ")\n",
    "\n",
    "graph_test = StellarGraph(\n",
    "    nodes=pd.DataFrame(index=graph_test_Stella.nodes()),\n",
    "    edges=edges_test,\n",
    "    edge_weight_column=\"time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-exercise",
   "metadata": {},
   "source": [
    "## Create Link Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-blood",
   "metadata": {},
   "source": [
    "### Upload Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_train_pos = pd.read_csv(\"/home/c6/Desktop/OpenWPM/jsons/CTDNE/2019/train/edges_train_label.csv\")\n",
    "edges_test_pos = pd.read_csv(\"/home/c6/Desktop/OpenWPM/jsons/CTDNE/2019/test/edges_train_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_and_negative_links(g, edges):\n",
    "    pos = list(edges[[\"source\", \"target\"]].itertuples(index=False))\n",
    "    neg = sample_negative_examples(g, pos)\n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "def sample_negative_examples(g, positive_examples):\n",
    "    positive_set = set(positive_examples)\n",
    "\n",
    "    def valid_neg_edge(src, tgt):\n",
    "        return (\n",
    "            # no self-loops\n",
    "            src != tgt\n",
    "            and\n",
    "            # neither direction of the edge should be a positive one\n",
    "            (src, tgt) not in positive_set\n",
    "            and (tgt, src) not in positive_set\n",
    "        )\n",
    "\n",
    "    possible_neg_edges = [\n",
    "        (src, tgt) for src in g.nodes() for tgt in g.nodes() if valid_neg_edge(src, tgt)\n",
    "    ]\n",
    "    return random.sample(possible_neg_edges, k=len(positive_examples))\n",
    "\n",
    "\n",
    "pos, neg = positive_and_negative_links(graph_train, edges_train_pos)\n",
    "pos_test, neg_test = positive_and_negative_links(graph_test, edges_test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-resolution",
   "metadata": {},
   "source": [
    "#### View Taining graph and positive and negative links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{graph_train.info()}\\n\"\n",
    "    f\"Training examples: {len(pos)} positive links, {len(neg)} negative links\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-gibraltar",
   "metadata": {},
   "source": [
    "#### View Test graph and positive and negative links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{graph_test.info()}\\n\"\n",
    "    f\"Test examples: {len(pos_test)} positive links, {len(neg_test)} negative links\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-matthew",
   "metadata": {},
   "source": [
    "## Running Random Walks\n",
    "\n",
    "Define the random walk parameters we'd like to use:\n",
    "\n",
    "    * num_walks_per_node - Number of random walks to perform per starting node.\n",
    "    * walk_length - Length of each random walk. For temporal walks, this is the maximum length of a walk, since         walks may end up being shorter when there are not enough time respecting edges to traverse.\n",
    "    * context_window_size - Size of the context window used to train the Word2Vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_walks_per_node = 10\n",
    "walk_length = 80\n",
    "context_window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cw = len(graph_train.nodes()) * num_walks_per_node * (walk_length - context_window_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.data import TemporalRandomWalk\n",
    "\n",
    "temporal_rw = TemporalRandomWalk(graph_train)\n",
    "temporal_walks = temporal_rw.run(\n",
    "    num_cw=num_cw,\n",
    "    cw_size=context_window_size,\n",
    "    max_walk_length=walk_length,\n",
    "    walk_bias=\"exponential\",\n",
    ")\n",
    "\n",
    "print(\"Number of temporal random walks: {}\".format(len(temporal_walks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 128\n",
    "# a trained model based on our graph which can produce embeddings\n",
    "temporal_model = Word2Vec(\n",
    "    temporal_walks,\n",
    "    size=embedding_size,\n",
    "    window=context_window_size,\n",
    "    min_count=0,\n",
    "    sg=1,\n",
    "    workers=2,\n",
    "    iter=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-presentation",
   "metadata": {},
   "source": [
    "For convenience, we can use the trained Word2Vec models to define helper functions that transform a node ID into a node embedding.\n",
    "\n",
    "NOTE: Temporal walks may not generate an embedding for every node in the graph; if there's no temporal walks that involve a particular node or they are all too short, the node won't appear in any context window. We handle this by using zeros as embeddings for such nodes to indicate that they are uninformative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_node_embedding = np.zeros(embedding_size)\n",
    "\n",
    "def temporal_embedding(u):\n",
    "    try:\n",
    "        return temporal_model.wv[u]\n",
    "    except KeyError:\n",
    "        return unseen_node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-adult",
   "metadata": {},
   "source": [
    "## Node Embedding Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-artist",
   "metadata": {},
   "source": [
    "For visualisation of embeddings, we'll first define a helper function that we can also use later to show the TSNE visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(title, x, y=None):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    x_t = tsne.fit_transform(x)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.title(title)\n",
    "    alpha = 0.7 if y is None else 0.5\n",
    "\n",
    "    scatter = plt.scatter(x_t[:, 0], x_t[:, 1], c=y, cmap=\"jet\", alpha=alpha)\n",
    "    if y is not None:\n",
    "        plt.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-wilson",
   "metadata": {},
   "source": [
    "We can visualise the node embeddings to take a glance at how the temporal walks have resulted in groups of nodes being clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_node_embeddings = temporal_model.wv.vectors\n",
    "plot_tsne(\"TSNE visualisation of temporal node embeddings\", temporal_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-destiny",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_model.wv.vectors[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-expert",
   "metadata": {},
   "source": [
    "## Link Prediction using Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-heather",
   "metadata": {},
   "source": [
    "The node embeddings we've generated can now be used as input for a link prediction task.\n",
    "\n",
    "Below are a set of helper functions we can use to train and evaluate a link prediction classifier. The rest of the notebook will use the binary operator defined in the cell below.\n",
    "\n",
    "Other commonly used binary operators are hadamard, average, and L1 operators. Alternatively, other user defined function taking two node embeddings to produce a link embedding could be used, but may affect convergence of the classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking two node embeddings to produce a link embedding (concatenate them using L2 operator)\n",
    "def operator_l2(u, v):\n",
    "    return (u - v) ** 2\n",
    "\n",
    "\n",
    "binary_operator = operator_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def link_examples_to_features(link_examples, transform_node):\n",
    "    op_func = (\n",
    "        operator_func[binary_operator]\n",
    "        if isinstance(binary_operator, str)\n",
    "        else binary_operator\n",
    "    )\n",
    "    return [\n",
    "        op_func(transform_node(src), transform_node(dst)) for src, dst in link_examples\n",
    "    ]\n",
    "\n",
    "\n",
    "def link_prediction_classifier(max_iter=2000):\n",
    "    # Cs describes the inverse of regularization strength\n",
    "    # cv number of cross folds\n",
    "    lr_clf = LogisticRegressionCV(Cs=10, cv=10, scoring=\"roc_auc\", max_iter=max_iter)\n",
    "    # steps List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator.\n",
    "    return Pipeline(steps=[(\"sc\", StandardScaler()), (\"clf\", lr_clf)])\n",
    "\n",
    "\n",
    "def evaluate_roc_auc(clf, link_features, link_labels):\n",
    "    predicted = clf.predict_proba(link_features)\n",
    "\n",
    "    # check which class corresponds to positive links\n",
    "    positive_column = list(clf.classes_).index(1)\n",
    "    return roc_auc_score(link_labels, predicted[:, positive_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-university",
   "metadata": {},
   "source": [
    "We'll create some positive and negative examples to train our classifier on. The negative examples can be randomly generated from the available nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_links(positive_examples, negative_examples):\n",
    "    return (\n",
    "        positive_examples + negative_examples,\n",
    "        np.repeat([1, 0], [len(positive_examples), len(negative_examples)]),\n",
    "    )\n",
    "\n",
    "\n",
    "link_examples, link_labels = labelled_links(pos, neg)\n",
    "link_examples_test, link_labels_test = labelled_links(pos_test, neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-termination",
   "metadata": {},
   "source": [
    "## Link prediction classifier using temporal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    " # reture a pipeline, The pipeline can be used as any other estimator and avoids leaking the test set into the train set\n",
    "temporal_clf = link_prediction_classifier()\n",
    "\n",
    "# convert all node Ids in training dataset to embeddings\n",
    "temporal_link_features = link_examples_to_features(link_examples, temporal_embedding)\n",
    "\n",
    "# convert all node Ids in test dataset to embeddings\n",
    "temporal_link_features_test = link_examples_to_features(link_examples_test, temporal_embedding)\n",
    "temporal_clf.fit(temporal_link_features, link_labels)\n",
    "temporal_score = evaluate_roc_auc(\n",
    "    temporal_clf, temporal_link_features_test, link_labels_test\n",
    ")\n",
    "\n",
    "print(f\"Score (ROC AUC): {temporal_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
